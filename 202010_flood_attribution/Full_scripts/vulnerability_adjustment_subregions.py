#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Jan 25 22:26:55 2020

@author: insauer
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
import numpy.ma as ma
from astropy.convolution import convolve
from scipy import stats
from pyts.decomposition import SingularSpectrumAnalysis
import scipy.stats as stats



def runmean(data, halfwin):
    """
    Simple running mean.
    CAUTION: Data is *extended* at the edges by repeating the
    edge values; thereby any trend present in the data will
    become attenuated at the edges!
    """
    window = 2*halfwin + 1
    if window > len(data):
        print('Error: window too large!')
        import sys
        sys.exit(0)
    weights = np.repeat(1.0, window) / window
    # Treat edges: Extend data
    extended_data = np.hstack([[data[0]] * (halfwin), data, [data[len(data)-1]]
                               * (halfwin)])
    # rm = np.convolve(extended_data, weights, 'valid')
    rm = convolve(extended_data, weights, boundary=None, nan_treatment='fill',
                  preserve_nan=False)
    return rm[halfwin:-halfwin]


def regression(data_frame, use_log, column_name, column_nameY):
    """
    This function adjusts for vulnerability, applying a GDP fit, either in the
    log space or the linear space. Not used in the paper output!
    Parameters
    ----------
    dataFrame : DataFrame
        regionally aggregated model medians
    use_log : bool
        if True apply logarithmic (not used)
    column_name : string
        independent regression variable
    column_nameY : string
        dependent regression variable

    Returns
    -------
    RegressionObject
        regionally aggregated damages and other indicators

    """
    if log_char == 'DLOG':
        rm_ratio = np.log10(data_frame[column_nameY].rolling(window=3,
                                                             min_periods=1).mean())
        rm_ratio = rm_ratio.replace([-np.inf, np.inf], [np.nan, np.nan])
        gdp = sm.tools.add_constant(np.log10(data_frame[column_name]),
                                    prepend=False)
        reg = sm.OLS(rm_ratio, gdp, missing='drop').fit()

    else:

        ratio = data_frame[column_nameY]
        gdp = sm.tools.add_constant(data_frame[column_name], prepend=False)
        reg = sm.GLM(ratio, gdp, missing='drop',
                     family=sm.families.Gaussian(sm.families.links.identity())).fit()

        # reg=sm.GLM(ratio, gdp, missing='drop',
        #       family=sm.families.Gamma(sm.families.links.identity())).fit()

    return reg


def get_rm(col_names, dataFrame, rm_window):
    """
    Function applies running mean on selected columns
    ----------
    ratio : Column of DataFrame
        Ratio of recorded to modeled damages

    Returns
    -------
    np.arrays
        Ratios with different window sizes

    """
    for col in col_names:
        dataFrame[col] = dataFrame[col].replace(
                                            [-np.inf, np.inf],
                                            [np.nan, np.nan])
        dataFrame[col] = runmean(dataFrame[col], rm_window)

    return dataFrame


def calc_cutoff(ratio):
    """
    Function calculated upper and lower cutoff values
    for outlier removal
    ----------
    ratio : Column of DataFrame
        Ratio of recorded to modeled damages

    Returns
    -------
    floats
        lower cutoff, upper cutoff 

    """
    q30, q70 = np.nanpercentile(ratio, 30), np.nanpercentile(ratio, 70)

    iqr = q70 - q30

    cut_off = iqr * 5

    lower, upper = q30 - cut_off, q70 + cut_off

    return lower, upper


def vul_funcs(ratio):
    """
    This functions estimates a vulnerability function, by flattening the ration
    of observed and modeled damages. Provided is a smoothing with different
    window-sizes for running means and a smoothing with the SSA tool. For
    further analysis only ssa_5 was considered (11-yr running mean)
    Parameters
    ----------
    ratio : Column of DataFrame
        Ratio of recorded to modeled damages

    Returns
    -------
    np.arrays
        Ratios with different window sizes

    """
    ratio3yr = runmean(np.array(ratio), 1)

    ratio5yr = runmean(np.array(ratio), 2)

    ratio7yr = runmean(np.array(ratio), 3)

    ratio9yr = runmean(np.array(ratio), 4)

    ratio_ssa = ratio.replace([np.nan], [ratio.median()])

    ratio_test = np.zeros((1, 31))

    ratio_test[0, :] = ratio_ssa

    ssa = SingularSpectrumAnalysis(window_size=11, groups=None)
    X_ssa5 = ssa.fit_transform(ratio_test)

    ssa = SingularSpectrumAnalysis(window_size=10, groups=None)
    X_ssa10 = ssa.fit_transform(ratio_test)

    ssa_5 = X_ssa5[0, :]
    ssa_10 = X_ssa10[0, :]

    return ratio3yr, ratio5yr, ratio7yr, ratio9yr, ssa_5, ssa_10


def vul_func_test(ratio, dis):
    """
    This functions estimates a vulnerability function, by flattening the ration
    of observed and modeled damages. Provided is a smoothing with different
    window-sizes for running means and a smoothing with the SSA tool. For
    further analysis only ssa_5 was considered (11-yr running mean)
    Parameters
    ----------
    ratio : Column of DataFrame
        Ratio of recorded to modeled damages

    Returns
    -------
    np.arrays
        Ratios with different window sizes

    """
    

    ratio11yr = ratio['ratios{}'.format(dis)].rolling(window=11, min_periods=5, center=True).mean()

    ratio_reg = ratio['ratios{}'.format(dis)].replace([np.nan], [ratio['ratios{}'.format(dis)].median()])

    ratio_test_reg = np.zeros((1, 31))

    ratio_test_reg[0, :] = ratio_reg

    ssa = SingularSpectrumAnalysis(window_size=11, groups=None)
    X_ssa5 = ssa.fit_transform(ratio_test_reg)

    ssa_5 = X_ssa5[0, :]

    return ratio11yr, ssa_5


def adjust_dam(data, vul_func, dis):
    """
    This functions adjusts modeled damages for vulnerability changes using
    vulnerability functions
    ----------
    data : DataFrame
        DataFrame with modeled damages
    vul_func : np.array
        time dependent vulnerability function
    dis : string
        discharge group

    Returns
    -------
    np.arrays
        damages after accounting for vulnerability, including onethird and two-
        third model quantiles

    """
    vul_func = vul_func/vul_func.max()

    predicted_damages = data['Impact_2y{}'.format(dis)] * vul_func
    predicted_damages = predicted_damages.replace([-np.inf, np.inf],
                                                  [np.nan, np.nan])

    predicted_damages_onethird = data['Impact_2y{}_onethird_quantile'.format(dis)] * vul_func
    predicted_damages_onethird = predicted_damages_onethird.replace([-np.inf, np.inf],
                                                                    [np.nan, np.nan])

    predicted_damages_twothird = data['Impact_2y{}_twothird_quantile'.format(dis)] * vul_func
    predicted_damages_twothird = predicted_damages_twothird.replace([-np.inf, np.inf],
                                                                    [np.nan, np.nan])

    return predicted_damages, predicted_damages_onethird,\
        predicted_damages_twothird


def pred_damages(reg, data, log_char, disch):
    """
    This functions adjusts modeled damages for vulnerability changes depending
    GDP ot time obtained from a regression.
    ----------
    reg : RegressionObject
        result from the vulnerability fit
    data : DataFrame
        DataFrame with modeled damages
    log_char : string
        double logarithmic or semi-logarithmic fit
    disch : string
        discharge group
    Returns
    -------
    np.arrays
        damages after accounting for vulnerability, including onethird and two-
        third model quantiles

    """
    if log_char == 'DLOG':

        predicted_damages = np.log(data['Impact_2y{}'.format(disch)]) - \
                (np.log10(data['GDP_pc{}'.format(disch)]) *
                 reg.params['GDP_pc{}'.format(disch)] + reg.params['const'])
        predicted_damages = predicted_damages.replace([-np.inf, np.inf],
                                                      [np.nan, np.nan])

        predicted_damages = np.exp(predicted_damages)

        predicted_damages_onethird = np.log10(data['Impact_2y{}_onethird_quantile'.format(disch)]) - \
                (np.log10(data['GDP_pc{}'.format(disch)])*reg.params['GDP_pc{}'.format(disch)]
                 + reg.params['const'])

        predicted_damages_onethird = predicted_damages_onethird.replace([-np.inf, np.inf],
                                                                        [np.nan, np.nan])

        predicted_damages_onethird = np.exp(predicted_damages_onethird)

        predicted_damages_twothird = np.log(data['Impact_2y{}_twothird_quantile'.format(disch)]) - \
            (np.log(data['GDP_pc{}'.format(disch)])*reg.params['GDP_pc{}'.format(disch)]
             + reg.params['const'])
        predicted_damages_twothird = predicted_damages_twothird.replace([-np.inf, np.inf],
                                                                        [np.nan, np.nan])

        predicted_damages_twothird = np.log(predicted_damages_twothird)

    else:

        predicted_damages = data['Impact_2y{}'.format(disch)] *\
          (data['GDP_pc{}'.format(disch)]*reg.params[0] + reg.params['const'])
        predicted_damages = predicted_damages.replace([-np.inf, np.inf],
                                                      [np.nan, np.nan])

        predicted_damages_onethird = data['Impact_2y{}_onethird_quantile'.format(disch)]\
                   * (data['GDP_pc{}'.format(disch)]*reg.params[0] + reg.params['const'])
        predicted_damages_onethird = predicted_damages_onethird.replace([-np.inf, np.inf],
                                                                        [np.nan, np.nan])

        predicted_damages_twothird = data['Impact_2y{}_twothird_quantile'.format(disch)]\
                   * (data['GDP_pc{}'.format(disch)]*reg.params[0] + reg.params['const'])
        predicted_damages_twothird = predicted_damages_twothird.replace([-np.inf, np.inf],
                                                                        [np.nan, np.nan])

    return predicted_damages, predicted_damages_onethird, predicted_damages_twothird


def corr_obs(corr_obs_ts, corr_ts, use_log):
    """
    Correlation of modeled and observed damages
    ----------
    corr_obs_ts : np.array
        observed damages
    corr_ts : np.array
        damages to be correlated
    use_log : string
        correlation in log space
    Returns
    -------
    CorrelationObject

    """
    if use_log:
        a = ma.masked_invalid(np.log10(corr_obs_ts).replace([-np.inf, np.inf],
                                                            [np.nan, np.nan]))
        b = ma.masked_invalid(np.log10(corr_ts))
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])

        # corrcoef = stats.spearmanr(a[msk], b[msk])

    else:
        a = ma.masked_invalid(corr_obs_ts.replace([-np.inf, np.inf],
                                                  [np.nan, np.nan]))
        b = ma.masked_invalid(corr_ts)
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])
        # corrcoef = stats.spearmanr(a[msk], b[msk])
        
    if np.ma.is_masked(corrcoef[0, 1]):
        return np.array([[1.0, 0.0], [0.0, 0.0]])

    return corrcoef


def corr_spear(corr_obs_ts, corr_ts, use_log):
    """
    Spearman-rank-correlation of modeled and observed damages
    ----------
    corr_obs_ts : np.array
        observed damages
    corr_ts : np.array
        damages to be correlated
    use_log : string
        correlation in log space
    Returns
    -------
    CorrelationObject

    """
    if use_log:
        a = ma.masked_invalid(np.log10(corr_obs_ts).replace([-np.inf, np.inf],
                                                            [np.nan, np.nan]))
        b = ma.masked_invalid(np.log10(corr_ts))
        msk = (~a.mask & ~b.mask)

        tau, p_value = stats.kendalltau(a[msk], b[msk])

    else:
        a = ma.masked_invalid(corr_obs_ts.replace([-np.inf, np.inf],
                                                  [np.nan, np.nan]))
        b = ma.masked_invalid(corr_ts)
        msk = (~a.mask & ~b.mask)

        tau, p_value = stats.kendalltau(a[msk], b[msk])

    return tau, p_value


def rm_pears_corr_obs(corr_obs_ts, corr_ts, use_log):
    """
    Pearson-Correlation of modeled and observed damages, applying a running
    mean before (3yr)
    ----------
    corr_obs_ts : np.array
        observed damages
    corr_ts : np.array
        damages to be correlated
    use_log : string
        correlation in log space
    Returns
    -------
    CorrelationObject

    """
    rm_obs = runmean(np.array(corr_obs_ts), 1)
    rm_ts = runmean(np.array(corr_ts), 1)

    if use_log:
        a = ma.masked_invalid(np.log10(rm_obs).replace([-np.inf, np.inf],
                                                       [np.nan, np.nan]))
        b = ma.masked_invalid(np.log10(rm_ts))
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])

        #  corrcoef = stats.spearmanr(a[msk], b[msk])

    else:
        a = ma.masked_invalid(rm_obs)
        b = ma.masked_invalid(rm_ts)
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])

        # corrcoef = stats.spearmanr(a[msk], b[msk])

    return corrcoef


def total_damages(data, disch='pos'):
    """
    Sums up annual damages to total damages
    ----------/home/insauer/projects/Attribution/Floods/Paper_NC_Review_Data/vulnerability_test/modeled/outlier_3
    data : DataFrame
        DataFrame with modeled damages

    Returns
    -------
    total_oberved_damages
    total_model_damages
    total_pred_damages

    """
    total_oberved_damages = np.sum(data['natcat_damages_2005_CPI_{}'.format(disch)].
                                  where(pd.notna(data['Impact_2y{}'.format(disch)])))
                                  
    total_model_damages = np.sum(data['Impact_2y{}'.format(disch)].
                               where(pd.notna(data['natcat_damages_2005_CPI_{}'.format(disch)])))
    total_pred_damages = np.sum(data['Impact_Pred_{}'.format(disch)].
                               where(pd.notna(data['natcat_damages_2005_CPI_{}'.format(disch)])))

    return total_oberved_damages, total_model_damages, total_pred_damages


def get_explained_variance(reg):
    """
    Calculate explained variance from deviance and null_deviance,
    included in regression result.
    This function is not used in the paper output!
    ----------
    data : DataFrame
        output from regression

    Returns
    -------
    explained variance

    """

    dev_const = reg.null_deviance
    dev_full = reg.deviance

    return 1-(dev_full/dev_const)


def arange_fit_info(region, corr_mod, corr_pred,
                    tot_obs_dam, tot_mod_dam, tot_pred_dam,
                    corr_clim, nan_ev,
                    annual_mean_obs, annual_std_obs, vul_mean, disch):
    """
    Prepares dataframe for file output

    Parameters
    ----------
    region : string
        shortage of region
    corr_mod : CorrelationObject
        pearson correlation coefficient (model/observed)
    corr_pred : CorrelationObject
        pearson correlation coefficient (vulnerability adjusted
                                         model/observed)
    tot_obs_dam : float
        total observed damages 1980-2010
    tot_mod_dam : float
        total modeled damages 1980-2010
    tot_pred_dam : float
        total vulnerability adjusted damages 1980-2010
    disch : string
        discharge region

    Returns
    -------
    DataFrame
        all output variables for one region

    """
    table1 = pd.DataFrame(data={'Region': region+'_' + disch,
                                'corrcoef_model_observed': corr_mod[0, 1],
                                'corrcoef_pred_observed': corr_pred[0, 1],
                                'corrcoef_clim_observed': corr_clim[0, 1],
                                'ExpVar_model_observed':
                                    corr_mod[0, 1] * corr_mod[0, 1],
                                'ExpVar_model_pred_observed':
                                    corr_pred[0, 1] * corr_pred[0, 1],
                                'ExpVar_model_clim_observed':
                                    corr_clim[0, 1] * corr_clim[0, 1],
                                'Observed_damages': tot_obs_dam,
                                'Model_damages': tot_mod_dam,
                                'Predicted_damages': tot_pred_dam,
                                'Annual mean': annual_mean_obs,
                                'Std dev': annual_std_obs,
                                'Mean vul': vul_mean,
                                'Nan events': nan_ev},
                          index=[0])

    return table1


def vul_fit(dataFrame, rm_columns=None):
    """
    Wrapper function for vulnerability estimation and correlation

    Parameters
    ----------
    dataFrame : DataFrame
        DataFrame containing full time series
    log_char : string
        Aplling logarithmic metrics
    x_dep : TYPE
        variable used for fit time or GDP (not relevant in current version)
    rm_columns : string list, optional
        columns selected for running-mean consideration

    Returns
    -------
    region_data : DataFrame
        All damage Time series including those with vulnerability estimates
    fit_data : DataFrame
        Other metrics, such as correlation, total damages...

    """

    runmean_window = 1

    region_data = pd.DataFrame()
    fit_data = pd.DataFrame()

    for i, test_region in enumerate(test_regions):

        DATA_region = dataFrame[(dataFrame['Region'] == test_region) &
                                (dataFrame['Year'] < 2011) &
                                (dataFrame['Year'] > 1970)]
        DATA_region = DATA_region.reset_index()

        if rm_columns:
            DATA_region = get_rm(rm_columns, DATA_region, runmean_window)

        # calculating vulnerability ratio
        DATA_region.loc[DATA_region['Year'] > 1979, 'ratiosPos'] = \
            DATA_region.loc[DATA_region['Year'] > 1979, 'natcat_damages_2005_CPI_Pos'] / \
            DATA_region.loc[DATA_region['Year'] > 1979, 'Impact_2yPos']
        DATA_region.loc[DATA_region['Year'] > 1979, 'ratiosNeg'] = \
            DATA_region.loc[DATA_region['Year'] > 1979, 'natcat_damages_2005_CPI_Neg'] / \
            DATA_region.loc[DATA_region['Year'] > 1979, 'Impact_2yNeg']

        DATA_region['ratiosPos'] = DATA_region['ratiosPos'].replace([-np.inf, np.inf, 0.0],
                                                                    [np.nan, np.nan, np.nan])
        DATA_region['ratiosNeg'] = DATA_region['ratiosNeg'].replace([-np.inf, np.inf, 0.0],
                                                                    [np.nan, np.nan, np.nan])
        
        # get outlier borders
        lowctf_pos, upctf_pos = calc_cutoff(DATA_region.loc[DATA_region['Year'] > 1979, 'ratiosPos'])
        lowctf_neg, upctf_neg = calc_cutoff(DATA_region.loc[DATA_region['Year'] > 1979, 'ratiosNeg'])
        
        # removed_yearsPos = DATA_region.loc[(DATA_region['ratiosPos'] > upctf_pos) |
        #                 (DATA_region['ratiosPos'] > 500),
        #                 'Year']
        
        # removed_yearsNeg = DATA_region.loc[(DATA_region['ratiosNeg'] > upctf_neg) |
        #                 (DATA_region['ratiosNeg'] > 500),
        #                 'Year']

        # remove outliers
        DATA_region.loc[(DATA_region['ratiosPos'] > upctf_pos) |
                        (DATA_region['ratiosPos'] > 500),
                        'ratiosPos'] = np.nan
        
        nan_evPos = DATA_region.loc[DATA_region['Year']>1979, 'ratiosPos'].isna().sum()

        DATA_region.loc[(DATA_region['ratiosNeg'] > upctf_neg) |
                        (DATA_region['ratiosNeg'] > 500),
                        'ratiosNeg'] = np.nan
        
        nan_evNeg = DATA_region.loc[DATA_region['Year']>1979, 'ratiosNeg'].isna().sum()
        
        # get vulnerability function
        vul_func11yrPos, ssa_5Pos = \
            vul_func_test(DATA_region.loc[DATA_region['Year'] > 1979, ['ratiosPos']],'Pos')
        vul_func11yrNeg, ssa_5Neg = \
            vul_func_test(DATA_region.loc[DATA_region['Year'] > 1979, ['ratiosNeg']],'Neg')

        # predict damage
        pred_dam_pos, pred_1thrd_pos, pred_2thrd_pos = \
            adjust_dam(DATA_region[DATA_region['Year'] > 1979], ssa_5Pos, 'Pos')
        pred_dam_neg, pred_1thrd_neg, pred_2thrd_neg = \
            adjust_dam(DATA_region[DATA_region['Year'] > 1979], ssa_5Neg, 'Neg')

        DATA_region.loc[DATA_region['Year'] > 1979,
                        'Impact_Pred_Pos'] = pred_dam_pos
        DATA_region.loc[DATA_region['Year'] > 1979,
                        'Impact_Pred_1thrd_Pos'] = pred_1thrd_pos
        DATA_region.loc[DATA_region['Year'] > 1979,
                        'Impact_Pred_2thrd_Pos'] = pred_2thrd_pos

        DATA_region.loc[DATA_region['Year'] > 1979,
                        'Impact_Pred_Neg'] = pred_dam_neg
        DATA_region.loc[DATA_region['Year'] > 1979,
                        'Impact_Pred_1thrd_Neg'] = pred_1thrd_neg
        DATA_region.loc[DATA_region['Year'] > 1979,
                        'Impact_Pred_2thrd_Neg'] = pred_2thrd_neg

        DATA_region.loc[DATA_region['Year'] > 1979,
                        'vul_func11yrPos'] = vul_func11yrPos
        DATA_region.loc[DATA_region['Year'] > 1979,
                        'vul_funcSSA5Pos'] = ssa_5Pos

        DATA_region.loc[DATA_region['Year'] > 1979,
                        'vul_func11yrNeg'] = vul_func11yrNeg
        DATA_region.loc[DATA_region['Year'] > 1979,
                        'vul_funcSSA5Neg'] = ssa_5Neg

        # total damage
        tot_obs_dam_pos, tot_mod_dam_pos, tot_pred_dam_pos = \
            total_damages(DATA_region[DATA_region['Year'] > 1979], 'Pos')
        tot_obs_dam_neg, tot_mod_dam_neg, tot_pred_dam_neg = \
            total_damages(DATA_region[DATA_region['Year'] > 1979], 'Neg')

        # get annual mean damage
        annual_mean_obs_pos = DATA_region.loc[DATA_region['Year'] > 1979,
                                              'natcat_damages_2005_CPI_Pos'].mean()
        annual_mean_obs_neg = DATA_region.loc[DATA_region['Year'] > 1979,
                                              'natcat_damages_2005_CPI_Neg'].mean()
        # get annual std. dev
        annual_std_obs_pos = DATA_region.loc[DATA_region['Year'] > 1979,
                                             'natcat_damages_2005_CPI_Pos'].std()
        annual_std_obs_neg = DATA_region.loc[DATA_region['Year'] > 1979,
                                             'natcat_damages_2005_CPI_Neg'].std()
        # mean vulnerability
        vul_mean_pos = DATA_region.loc[DATA_region['Year'] > 1979, 'ratiosPos'].mean()

        vul_mean_neg = DATA_region.loc[DATA_region['Year'] > 1979, 'ratiosNeg'].mean()

        DATA_region90 = DATA_region[(DATA_region['Year']<2011) &
                                    (DATA_region['Year']>1989)]
        DATA_region90 = DATA_region90.reset_index()

        # get correlation
        corr_clim_pos = corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                                 'natcat_damages_2005_CPI_Pos'],
                                 DATA_region.loc[DATA_region['Year'] > 1979,
                                                 'ImpFix_2yPos'], use_log=False)
        corr_clim_neg = corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                                 'natcat_damages_2005_CPI_Neg'],
                                 DATA_region.loc[DATA_region['Year'] > 1979,
                                                 'ImpFix_2yNeg'], use_log=False)

        corr_mod_pos = corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                                'natcat_damages_2005_CPI_Pos'],
                                DATA_region.loc[DATA_region['Year'] > 1979,
                                                'Impact_2yPos'], use_log=False)
        corr_mod_neg = corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                                'natcat_damages_2005_CPI_Neg'],
                                DATA_region.loc[DATA_region['Year'] > 1979,
                                                'Impact_2yNeg'], use_log=False)

        corr_pred_pos = corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                                 'natcat_damages_2005_CPI_Pos'],
                                 DATA_region.loc[DATA_region['Year'] > 1979,
                                                 'Impact_Pred_Pos'],
                                 use_log=False)
        corr_pred_neg = corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                                 'natcat_damages_2005_CPI_Neg'],
                                 DATA_region.loc[DATA_region['Year'] > 1979,
                                                 'Impact_Pred_Neg'],
                                 use_log=False)

        fit_info_pos = arange_fit_info(test_region, corr_mod_pos,
                                       corr_pred_pos,
                                       tot_obs_dam_pos,
                                       tot_mod_dam_pos, tot_pred_dam_pos,
                                       corr_clim_pos, nan_evPos,
                                       annual_mean_obs_pos,
                                       annual_std_obs_pos,
                                       vul_mean_pos,
                                       'Pos')
        fit_info_neg = arange_fit_info(test_region, corr_mod_neg,
                                       corr_pred_neg,
                                       tot_obs_dam_neg,
                                       tot_mod_dam_neg, tot_pred_dam_neg,
                                       corr_clim_neg, nan_evNeg,
                                       annual_mean_obs_neg,
                                       annual_std_obs_neg,
                                       vul_mean_neg,
                                       'Neg')

        region_data = region_data.append(DATA_region, ignore_index=True)
        fit_data = fit_data.append(fit_info_pos, ignore_index=True)
        fit_data = fit_data.append(fit_info_neg, ignore_index=True)

    return region_data, fit_data


def add_global(time_series):
    """
    Aggregates global data to provide GLB as additional region

    Parameters
    ----------
    time_series : DataFrame
        Time series of all damages

    Returns
    -------
    time_series : DataFrame
        Damage time series including GLB

    """
    years = np.arange(1971, 2011)
    for yr in years:

        impPos = time_series.loc[(time_series['Year'] == yr),
                                 'Impact_2yPos'].sum()
        impNeg = time_series.loc[(time_series['Year'] == yr),
                                 'Impact_2yNeg'].sum()

        imp_1980Pos = time_series.loc[(time_series['Year'] == yr),
                                      'ImpFix_2yPos'].sum()
        imp_1980Neg = time_series.loc[(time_series['Year'] == yr),
                                      'ImpFix_2yNeg'].sum()

        imp_2010Pos = time_series.loc[(time_series['Year'] == yr),
                                      'Imp2010_2yPos'].sum()
        imp_2010Neg = time_series.loc[(time_series['Year'] == yr),
                                      'Imp2010_2yNeg'].sum()

        nat_catPos = time_series.loc[(time_series['Year'] == yr),
                                     'natcat_damages_2005_CPI_Pos'].sum()
        nat_catNeg = time_series.loc[(time_series['Year'] == yr),
                                     'natcat_damages_2005_CPI_Neg'].sum()

        area_1_3rdPos = time_series.loc[(time_series['Year'] == yr),
                                        'flood_area_onethird_quantile_Pos'].sum()
        area_1_3rdNeg = time_series.loc[(time_series['Year'] == yr),
                                        'flood_area_onethird_quantile_Neg'].sum()

        area_2_3rdPos = time_series.loc[(time_series['Year'] == yr),
                                        'flood_area_twothird_quantile_Pos'].sum()
        area_2_3rdNeg = time_series.loc[(time_series['Year'] == yr),
                                        'flood_area_twothird_quantile_Neg'].sum()

        imp_1_3rdPos = time_series.loc[(time_series['Year'] == yr),
                                       'Impact_2yPos_onethird_quantile'].sum()
        imp_1_3rdNeg = time_series.loc[(time_series['Year'] == yr),
                                       'Impact_2yNeg_onethird_quantile'].sum()

        imp_1980_1_3rdPos = time_series.loc[(time_series['Year'] == yr),
                                            'ImpFix_2yPos_onethird_quantile'].sum()
        imp_1980_1_3rdNeg = time_series.loc[(time_series['Year'] == yr),
                                            'ImpFix_2yNeg_onethird_quantile'].sum()
        imp_2010_1_3rdPos = time_series.loc[(time_series['Year'] == yr),
                                            'Imp2010_2yPos_onethird_quantile'].sum()
        imp_2010_1_3rdNeg = time_series.loc[(time_series['Year'] == yr),
                                            'Imp2010_2yNeg_onethird_quantile'].sum()
        imp_2_3rdPos = time_series.loc[(time_series['Year'] == yr),
                                       'Impact_2yPos_twothird_quantile'].sum()
        imp_2_3rdNeg = time_series.loc[(time_series['Year'] == yr),
                                       'Impact_2yNeg_twothird_quantile'].sum()

        imp_1980_2_3rdPos = time_series.loc[(time_series['Year'] == yr),
                                            'ImpFix_2yPos_twothird_quantile'].sum()
        imp_1980_2_3rdNeg = time_series.loc[(time_series['Year'] == yr),
                                            'ImpFix_2yNeg_twothird_quantile'].sum()
        imp_2010_2_3rdPos = time_series.loc[(time_series['Year'] == yr),
                                            'Imp2010_2yPos_twothird_quantile'].sum()
        imp_2010_2_3rdNeg = time_series.loc[(time_series['Year'] == yr),
                                            'Imp2010_2yNeg_twothird_quantile'].sum()
        areaPos = time_series.loc[(time_series['Year'] == yr),
                                  'FloodedAreaPos'].sum()
        areaNeg = time_series.loc[(time_series['Year'] == yr),
                                  'FloodedAreaNeg'].sum()
        time_series = time_series.append({'Year': yr,
                                          'Region': 'GLB',
                                          'Impact_2yPos': impPos,
                                          'Impact_2yNeg': impNeg,
                                          'ImpFix_2yPos': imp_1980Pos,
                                          'ImpFix_2yNeg': imp_1980Neg,
                                          'Imp2010_2yPos': imp_2010Pos,
                                          'Imp2010_2yNeg': imp_2010Neg,
                                          'natcat_damages_2005_CPI_Pos': nat_catPos,
                                          'natcat_damages_2005_CPI_Neg': nat_catNeg,
                                          'flood_area_onethird_quantile_Pos': area_1_3rdPos,
                                          'flood_area_onethird_quantile_Neg': area_1_3rdNeg,
                                          'flood_area_twothird_quantile_Pos': area_2_3rdPos,
                                          'flood_area_twothird_quantile_Neg': area_2_3rdNeg,
                                          'Impact_2yPos_onethird_quantile': imp_1_3rdPos,
                                          'Impact_2yNeg_onethird_quantile': imp_1_3rdNeg,
                                          'ImpFix_2yPos_onethird_quantile': imp_1980_1_3rdPos,
                                          'ImpFix_2yNeg_onethird_quantile': imp_1980_1_3rdNeg,
                                          'Imp2010_2yPos_onethird_quantile': imp_2010_1_3rdPos,
                                          'Imp2010_2yNeg_onethird_quantile': imp_2010_1_3rdNeg,
                                          'Impact_2yPos_twothird_quantile': imp_2_3rdPos,
                                          'Impact_2yNeg_twothird_quantile': imp_2_3rdNeg,
                                          'ImpFix_2yPos_twothird_quantile': imp_1980_2_3rdPos,
                                          'ImpFix_2yNeg_twothird_quantile': imp_1980_2_3rdNeg,
                                          'Imp2010_2yPos_twothird_quantile': imp_2010_2_3rdPos,
                                          'Imp2010_2yNeg_twothird_quantile': imp_2010_2_3rdNeg,
                                          'FloodedAreaPos': areaPos,
                                          'FloodedAreaNeg': areaNeg
                                          }, ignore_index=True)
    return time_series

DATA = pd.read_csv('/home/insauer/projects/NC_Submission/Data/postprocessing/ModelMedianSubregions.csv')


REGIONS = {'NAM': 'North America',
           'LAM': 'Latin America',
           'EUR': 'Western Europe',
           'NAFARA': 'North Africa + Middle East',
           'SSAF': 'Sub-Saharan Africa + Southern Africa',
           'CAS': 'Central Asia + Eastern Europe',
           'SWEA': 'Southern Asia + South-East Asia',
           'CHN': 'Eastern Asia',
           'AUS': 'Oceania',
           'GLB': 'Global'}


test_regions = list(REGIONS)


full_ts = add_global(DATA)

region_data_ts, fit_data = vul_fit(full_ts)

region_data_ts.to_csv('/home/insauer/projects/NC_Submission/Data/postprocessing/VulnerabilityAdjustmentTimeSeriesSubregions.csv', index=False)
fit_data.to_csv('/home/insauer/projects/NC_Submission/Data/postprocessing/VulnerabilityAdjustmentMetaDataSubregions.csv' , index=False)