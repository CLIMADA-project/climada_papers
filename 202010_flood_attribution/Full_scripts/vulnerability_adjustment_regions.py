#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Aug 25 15:21:06 2020

@author: insauer
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
import numpy.ma as ma
from astropy.convolution import convolve
from scipy import stats

from pyts.decomposition import SingularSpectrumAnalysis


def runmean(data, halfwin):
    """
    Simple running mean.
    CAUTION: Data is *extended* at the edges by repeating the
    edge values; thereby any trend present in the data will
    become attenuated at the edges!
    """
    window = 2*halfwin + 1
    if window > len(data):
        print('Error: window too large!')
        import sys
        sys.exit(0)
    weights = np.repeat(1.0, window) / window
    #  Treat edges: Extend data
    extended_data = np.hstack([[data[0]] * (halfwin), data, [data[len(data)-1]]
                               * (halfwin)])
    # rm = np.convolve(extended_data, weights, 'valid')
    rm = convolve(extended_data, weights, boundary=None, nan_treatment='fill',
                  preserve_nan=False)
    return rm[halfwin:-halfwin]


def regression(data_frame, use_log, log_char, column_name, column_nameY):
    """
    This function adjusts for vulnerability, applying a GDP fit, either in the
    log space or the linear space. Not used in the paper output!
    Parameters
    ----------
    dataFrame : DataFrame
        regionally aggregated model medians
    use_log : bool
        if True apply logarithmic (not used)
    column_name : string
        independent regression variable
    column_nameY : string
        dependent regression variable

    Returns
    -------
    RegressionObject
        regionally aggregated damages and other indicators

    """
    if log_char == 'DLOG':

        rm_ratio = np.log10(data_frame[column_nameY].rolling(window=3,
                                                             min_periods=1).mean())
        rm_ratio = rm_ratio.replace([-np.inf, np.inf], [np.nan, np.nan])

        gdp = sm.tools.add_constant(np.log10(data_frame[column_name]),
                                    prepend=False)
        reg = sm.OLS(rm_ratio, gdp, missing='drop').fit()

    else:

        ratio = data_frame[column_nameY]
        gdp = sm.tools.add_constant(data_frame['GDP_pc'], prepend=False)

        reg = sm.GLM(ratio, gdp, missing='drop',
              family=sm.families.Gamma(sm.families.links.identity())).fit()

    return reg


def vul_func(ratio):
    """
    This functions estimates a vulnerability function, by flattening the ration
    of observed and modeled damages. Provided is a smoothing with different
    window-sizes for running means and a smoothing with the SSA tool. For
    further analysis only ssa_5 was considered (11-yr running mean)
    Parameters
    ----------
    ratio : Column of DataFrame
        Ratio of recorded to modeled damages

    Returns
    -------
    np.arrays
        Ratios with different window sizes

    """

    ratio11yr = ratio['ratios'].rolling(window=11, min_periods=5, center=True).mean()

    ratio_reg = ratio['ratios'].replace([np.nan], [ratio['ratios'].median()])

    ratio_test_reg = np.zeros((1, 31))

    ratio_test_reg[0, :] = ratio_reg

    ssa = SingularSpectrumAnalysis(window_size=11, groups=None)
    X_ssa5 = ssa.fit_transform(ratio_test_reg)

    ssa_5 = X_ssa5[0, :]

    return ratio11yr, ssa_5


def normalised_corr(dataFrame, tot_mod_dam, tot_pred_dam):
    """
    This function adjusts for vulnerability, applying a GDP fit, either in the
    log space or the linear space. All relevant columns are normalised before
    they are correlated.
    Parameters
    ----------
    ratio : Column of DataFrame
        Ratio of recorded to modeled damages

    Returns
    -------
    np.arrays
        Ratios with different window sizes

    """
    facE = tot_pred_dam/tot_mod_dam
    facV = tot_pred_dam/tot_pred_dam
    facNatCat = tot_pred_dam/dataFrame['natcat_flood_damages_2005_CPI'].sum()

    pred_norm = dataFrame['Impact_Pred'] * facV
    mod_norm = dataFrame['Impact_2y_Flopros'] * facE
    natCat_norm = dataFrame['natcat_flood_damages_2005_CPI'] * facNatCat

    a = ma.masked_invalid(natCat_norm.replace([-np.inf, np.inf],
                                              [np.nan, np.nan]))
    b = ma.masked_invalid(pred_norm)
    msk = (~a.mask & ~b.mask)
    pred_corrcoef = ma.corrcoef(a[msk], b[msk])

    a = ma.masked_invalid(natCat_norm.replace([-np.inf, np.inf],
                                              [np.nan, np.nan]))
    b = ma.masked_invalid(mod_norm)
    msk = (~a.mask & ~b.mask)
    mod_corrcoef = ma.corrcoef(a[msk], b[msk])

    return pred_corrcoef, mod_corrcoef


def get_rm(col_names, dataFrame, rm_window):
    """
    Function applies running mean on selected columns
    ----------
    col_names : string list
        Columns to be smoothed
    dataFrame : DataFrame
        DataFrame that contains columns to be smoothed
    rm_window : int
        window size for running mean

    Returns
    -------
    DataFrame
        DataFrame with smoothed columns

    """
    for col in col_names:
        dataFrame[col] = dataFrame[col].replace(
                                            [-np.inf, np.inf],
                                            [np.nan, np.nan])
        dataFrame[col] = runmean(dataFrame[col], rm_window)

    return dataFrame


def calc_cutoff(ratio):

    q30, q70 = np.nanpercentile(ratio, 30), np.nanpercentile(ratio, 70)

    iqr = q70 - q30

    cut_off = iqr * 5

    lower, upper = q30 - cut_off, q70 + cut_off

    return lower, upper


def pred_damages(reg, data, log_char, x_dep):
    """
    This functions adjusts modeled damages for vulnerability changes depending
    GDP ot time obtained from a regression.
    ----------
    reg : RegressionObject
        result from the vulnerability fit
    data : DataFrame
        DataFrame with modeled damages
    log_char : string
        double logarithmic or semi-logarithmic fit
    x_dep : string
        independance variable (Year, GDPpc)
    Returns
    -------
    np.arrays
        damages after accounting for vulnerability, including onethird and two-
        third model quantiles

    """
    if log_char == 'DLOG':

        predicted_damages = np.log(data['Impact_2y_Flopros']) - \
                (np.log(data[x_dep])*reg.params[x_dep] + reg.params['const'])
        predicted_damages = predicted_damages.replace([-np.inf, np.inf],
                                                      [np.nan, np.nan])
        predicted_damages = np.exp(predicted_damages)

        predicted_damages_onethird = np.log(data['model_flood_damages_onethird_quantile']) - \
            (np.log(data[x_dep]) * reg.params[x_dep] + reg.params['const'])
        predicted_damages_onethird = predicted_damages_onethird.replace([-np.inf, np.inf],
                                                                        [np.nan, np.nan])

        predicted_damages_onethird = np.exp(predicted_damages_onethird)

        predicted_damages_twothird = np.log(data['model_flood_damages_twothird_quantile']) - \
            (np.log(data[x_dep]) * reg.params[x_dep] + reg.params['const'])
        predicted_damages_twothird = predicted_damages_twothird.replace([-np.inf, np.inf],
                                                                        [np.nan, np.nan]) 

        predicted_damages_twothird = np.exp(predicted_damages_twothird)

    else:

        predicted_damages = data['Impact_2y_Flopros'] * (data[x_dep]
                                                         * reg.params[0]
                                                         + reg.params['const'])
        predicted_damages = predicted_damages.replace([-np.inf, np.inf],
                                                      [np.nan, np.nan])

        # predicted_damages = np.exp(predicted_damages)

        predicted_damages_onethird = data['model_flood_damages_onethird_quantile']\
            * (data[x_dep]*reg.params[0] + reg.params['const'])
        predicted_damages_onethird = predicted_damages_onethird.replace([-np.inf, np.inf],
                                                                        [np.nan, np.nan])
        predicted_damages_onethird = predicted_damages_onethird

        predicted_damages_twothird = data['model_flood_damages_twothird_quantile']\
            * (data[x_dep]*reg.params[0] + reg.params['const'])
        predicted_damages_twothird = predicted_damages_twothird.replace([-np.inf, np.inf],
                                                                        [np.nan, np.nan])

        predicted_damages_twothird = predicted_damages_twothird

    return predicted_damages, predicted_damages_onethird, predicted_damages_twothird


def pears_corr_obs(corr_obs_ts, corr_ts, use_log):
    """
    Pearson-Correlation of modeled and observed damages
    ----------
    corr_obs_ts : np.array
        observed damages
    corr_ts : np.array
        damages to be correlated
    use_log : string
        correlation in log space
    Returns
    -------
    CorrelationObject

    """
    if use_log:
        a = ma.masked_invalid(np.log10(corr_obs_ts).replace([-np.inf, np.inf],
                                                            [np.nan, np.nan]))
        b = ma.masked_invalid(np.log10(corr_ts))
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])

        # corrcoef = stats.spearmanr(a[msk], b[msk])

    else:
        a = ma.masked_invalid(corr_obs_ts.replace([-np.inf, np.inf],
                                                  [np.nan, np.nan]))
        b = ma.masked_invalid(corr_ts)
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])

        #  corrcoef = stats.spearmanr(a[msk], b[msk])

    return corrcoef


def rm_pears_corr_obs(corr_obs_ts, corr_ts, use_log):
    """
    Pearson-Correlation of modeled and observed damages, applying a running
    mean before (3yr)
    ----------
    corr_obs_ts : np.array
        observed damages
    corr_ts : np.array
        damages to be correlated
    use_log : string
        correlation in log space
    Returns
    -------
    CorrelationObject

    """
    rm_obs = runmean(np.array(corr_obs_ts), 1)
    rm_ts = runmean(np.array(corr_ts), 1)

    if use_log:
        a = ma.masked_invalid(np.log10(rm_obs).replace([-np.inf, np.inf],
                                                       [np.nan, np.nan]))
        b = ma.masked_invalid(np.log10(rm_ts))
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])

        #  corrcoef = stats.spearmanr(a[msk], b[msk])

    else:
        a = ma.masked_invalid(rm_obs)
        b = ma.masked_invalid(rm_ts)
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])

        # corrcoef = stats.spearmanr(a[msk], b[msk])

    return corrcoef


def spear_corr_obs(corr_obs_ts, corr_ts, use_log):
    """
    Spearman-Rank-Correlation of modeled and observed damages, applying a
    running mean before (3yr)
    ----------
    corr_obs_ts : np.array
        observed damages
    corr_ts : np.array
        damages to be correlated
    use_log : string
        correlation in log space
    Returnsf3_ax1.scatter(DATA_regionFull['Year'], DATA_regionFull['ratios'], label='Ratio', color='black', marker = 'o', s = 2)
    -------
    CorrelationObject

    """
    if use_log:
        a = ma.masked_invalid(np.log10(corr_obs_ts).replace([-np.inf, np.inf],
                                                            [np.nan, np.nan]))
        b = ma.masked_invalid(np.log10(corr_ts))
        msk = (~a.mask & ~b.mask)

        tau, p_value = stats.kendalltau(a[msk], b[msk])

    else:
        a = ma.masked_invalid(corr_obs_ts.replace([-np.inf, np.inf],
                                                  [np.nan, np.nan]))
        b = ma.masked_invalid(corr_ts)
        msk = (~a.mask & ~b.mask)

        tau, p_value = stats.kendalltau(a[msk], b[msk])

    return tau, p_value


def adjust_dam(data, vul_func):
    """
    This function adjusts modeled damages for vulnerability changes using
    vulnerability functions
    ----------
    data : DataFrame
        DataFrame with modeled damages
    vul_func : np.array
        time dependent vulnerability function


    Returns
    -------
    np.arrays
        damages after accounting for vulnerability, including onethird and two-
        third model quantiles

    """
    vul_func = vul_func/vul_func.max()

    predicted_damages = data['Impact_2y_Flopros'] * vul_func
    predicted_damages = predicted_damages.replace([-np.inf, np.inf],
                                                  [np.nan, np.nan])

    predicted_damages_onethird = data['model_flood_damages_onethird_quantile'] * vul_func
    predicted_damages_onethird = predicted_damages_onethird.replace([-np.inf, np.inf],
                                                                    [np.nan, np.nan])

    predicted_damages_twothird = data['model_flood_damages_twothird_quantile'] * vul_func
    predicted_damages_twothird = predicted_damages_twothird.replace([-np.inf, np.inf],
                                                                    [np.nan, np.nan])

    return predicted_damages, predicted_damages_onethird, predicted_damages_twothird


def adjust_dam_linear(data, trend_info):
    """
    This function adjusts modeled damages for vulnerability changes using
    the paramter from the sen-slope
    ----------
    data : DataFrame
        DataFrame with modeled damages
    trend_info: np.array
        parameters from the TheilSenSlope estimation


    Returns
    -------
    np.arrays
        damages after accounting for vulnerability, including onethird and two-
        third model quantiles

    """
    vul_func = np.arange(len(data.loc[data['Year']>1979,'Impact_2y_Flopros']))*trend_info[0]+ trend_info[1]
    vul_func = vul_func/vul_func.max()

    prediction = data['Impact_2y_Flopros'] * vul_func

    return prediction


def total_damages(data):
    """
    Sums up annual damages to total damages
    ----------
    data : DataFrame
        DataFrame with modeled damages

    Returns
    -------
    total_oberved_damages
    total_model_damages
    total_pred_damages

    """
    total_oberved_damages = np.sum(data['natcat_flood_damages_2005_CPI'].
                                   where(pd.notna(data['Impact_2y_Flopros'])))
    total_model_damages = np.sum(data['Impact_2y_Flopros'].
                             where(pd.notna(data['natcat_flood_damages_2005_CPI'])))
    total_pred_damages = np.sum(data['Impact_Pred'].
                               where(pd.notna(data['natcat_flood_damages_2005_CPI'])))

    return total_oberved_damages, total_model_damages, total_pred_damages


def get_explained_variance(reg):
    """
    Calculate explained variance from deviance and null_deviance,
    included in regression result.
    This function is not used in the paper output!
    ----------
    data : DataFrame
        output from regression

    Returns
    -------
    explained variance

    """
    dev_const = reg.null_deviance
    dev_full = reg.deviance

    return 1-(dev_full/dev_const)


def arange_fit_info(region,
                    p_corr_mod, p_corr_pred, p_corr_clim,
                    tot_obs_dam, tot_mod_dam, tot_pred_dam,
                    annual_mean, annual_std, vul_mean, nan_ev):
    """Prepares dataframe for file output
    ----------
    region : string
         shortage of region
    p_corr_mod : CorrelationObject
         pearson correlation coefficient (model/observed)
    p_corr_pred : CorrelationObject
         pearson correlation coefficient (vulnerability adjusted
                                         model/observed)
    tot_obs_dam : float
         total observed damages 1980-2010
    tot_mod_dam : float/home/insauer/projects/Attribution/Floods/Paper_NC_Resubmission_data/Supplement
         total modeled damages 1980-2010
    tot_pred_dam : float
         total vulnerability adjusted damages 1980-2010

    Returns
    -------
    DataFrame
        all output variables for one region

    """

    table1 = pd.DataFrame(data={'Region': region,
                                'P_corrcoef_model_observed': p_corr_mod[0, 1],
                                'P_corrcoef_pred_observed': p_corr_pred[0, 1],
                                'P_corrcoef_clim_observed': p_corr_pred[0, 1],
                                'P_ExpVar_model_observed':
                                    p_corr_mod[0, 1] * p_corr_mod[0, 1],
                                'P_ExpVar_pred_observed':
                                    p_corr_pred[0, 1] * p_corr_pred[0, 1],
                                'P_ExpVar_clim_observed':
                                    p_corr_clim[0, 1] * p_corr_clim[0, 1],
                                'Observed_damages': tot_obs_dam,
                                'Model_damages': tot_mod_dam,
                                'Predicted_damages': tot_pred_dam,
                                'Annual mean': annual_mean,
                                'Std dev': annual_std,
                                'Mean vul': vul_mean
                                }, index=[0])

    return table1


def add_global(time_series):
    """
    Aggregates global data to provide GLB as additional region

    Parameters
    ----------
    time_series : DataFrame
        Time series of all damages

    Returns
    -------
    time_series : DataFrame
        Damage time series including GLB

    """
    years = np.arange(1971, 2011)
    for yr in years:

        imp = time_series.loc[(time_series['Year'] == yr),
                              'Impact_2y_Flopros'].sum()
        imp_1980 = time_series.loc[(time_series['Year'] == yr),
                                   'ImpFix_2y_Flopros'].sum()
        imp_2010 = time_series.loc[(time_series['Year'] == yr),
                                   'Imp2010_2y_Flopros'].sum()

        nat_cat = time_series.loc[(time_series['Year'] == yr),
                                  'natcat_flood_damages_2005_CPI'].sum()

        area_1_3rd = time_series.loc[(time_series['Year'] == yr),
                                     'flood_area_onethird_quantile'].sum()
        area_2_3rd = time_series.loc[(time_series['Year'] == yr),
                                     'flood_area_twothird_quantile'].sum()
        imp_1_3rd = time_series.loc[(time_series['Year'] == yr),
                                    'model_flood_damages_onethird_quantile'].sum()
        imp_1980_1_3rd = time_series.loc[(time_series['Year'] == yr),
                                         'model_flood_damages_onethird_quantile_1980flopros'].sum()
        imp_2010_1_3rd = time_series.loc[(time_series['Year'] == yr),
                                         'model_flood_damages_onethird_quantile_2010flopros'].sum()

        imp_2_3rd = time_series.loc[(time_series['Year'] == yr),
                                    'model_flood_damages_twothird_quantile'].sum()
        imp_1980_2_3rd = time_series.loc[(time_series['Year'] == yr),
                                         'model_flood_damages_twothird_quantile_1980flopros'].sum()
        imp_2010_2_3rd = time_series.loc[(time_series['Year'] == yr),
                                         'model_flood_damages_twothird_quantile_2010flopros'].sum()

        area = time_series.loc[(time_series['Year'] == yr) &
                               (time_series['Region'] != 'GLB'),
                               'FloodedAreaFlopros'].sum()
        time_series = time_series.append({'Year': yr,
                                          'Region': 'GLB',
                                          'Impact_2y_Flopros': imp,
                                          'ImpFix_2y_Flopros': imp_1980,
                                          'Imp2010_2y_Flopros': imp_2010,
                                          'natcat_flood_damages_2005_CPI':
                                              nat_cat,
                                          'flood_area_onethird_quantile':
                                              area_1_3rd,
                                          'flood_area_twothird_quantile':
                                              area_2_3rd,
                                          'model_flood_damages_onethird_quantile':
                                              imp_1_3rd,
                                          'model_flood_damages_onethird_quantile_1980flospros':
                                              imp_1980_1_3rd,
                                          'model_flood_damages_onethird_quantile_2010flospros':
                                              imp_2010_1_3rd,
                                          'model_flood_damages_twothird_quantile':
                                              imp_2_3rd,
                                          'model_flood_damages_twothird_quantile_1980flospros':
                                              imp_1980_2_3rd,
                                          'model_flood_damages_twothird_quantile_2010flospros':
                                              imp_2010_2_3rd,
                                          'FloodedAreaFlopros': area
                                          }, ignore_index=True)
    return time_series


def vul_fit(dataFrame, rm_columns=None):
    """
    Wrapper function for vulnerability estimation and correlation

    Parameters
    ----------
    dataFrame : DataFrame
        DataFrame containing full time series
    log_char : string
        Aplling logarithmic metrics
    x_dep : TYPE
        variable used for fit time or GDP (not relevant in current version)
    rm_columns : string list, optional
        columns selected for running-mean consideration

    Returns
    -------
    region_data : DataFrame
        All damage Time series including those with vulnerability estimates
    fit_data : DataFrame
        Other metrics, such as correlation, total damages...

    """

    region_data = pd.DataFrame()
    fit_data = pd.DataFrame()

    add_rem_c = 0

    for i, test_region in enumerate(test_regions):

        DATA_region = dataFrame[(dataFrame['Region'] == test_region) &
                                (dataFrame['Year'] < 2011) &
                                (dataFrame['Year'] > 1970)]
        DATA_region = DATA_region.reset_index()

        # get vulnerability ratios

        DATA_region.loc[DATA_region['Year'] > 1979, 'ratios'] = \
            DATA_region.loc[DATA_region['Year'] > 1979,
                            'natcat_flood_damages_2005_CPI'] / \
            DATA_region.loc[DATA_region['Year'] > 1979, 'Impact_2y_Flopros']
        DATA_region['ratios'] = DATA_region['ratios'].replace([-np.inf, np.inf, 0.0],
                                                              [np.nan, np.nan, np.nan])
        lowctf, upctf = calc_cutoff(DATA_region.loc[DATA_region['Year'] > 1979, 'ratios'])

        DATA_region.loc[(DATA_region['ratios'] > upctf),
                        'ratios'] = np.nan

        # remove events that were classified as outliers in the full region

        add_rem_regions = ['EUR', 'CAS', 'AUS']
        add_rem_events = [[2002, 2007], [2002], [1980, 1985, 1986, 1988, 1989, 2010]]

        if test_region in add_rem_regions:
            DATA_region.loc[DATA_region['Year'].isin(add_rem_events[add_rem_c]), 'ratios'] = np.nan
            add_rem_c += 1
        # count nan evnts
        nan_ev = DATA_region.loc[DATA_region['Year'] > 1979, 'ratios'].isna().sum()

        vul_func11yr, ssa_5 = vul_func(DATA_region.loc[DATA_region['Year'] > 1979, ['ratios']])

        pred_dam, pred_1thrd, pred_2thrd = \
            adjust_dam(DATA_region[DATA_region['Year'] > 1979], ssa_5)

        pred_dam_mova, pred_1thrd_mova, pred_2thrd_mova = \
            adjust_dam(DATA_region[DATA_region['Year'] > 1979], vul_func11yr)

        DATA_region.loc[DATA_region['Year'] > 1979, 'Impact_Pred'] = pred_dam
        DATA_region.loc[DATA_region['Year'] > 1979, 'Impact_Pred_1thrd'] = pred_1thrd
        DATA_region.loc[DATA_region['Year'] > 1979, 'Impact_Pred_2thrd'] = pred_2thrd
        DATA_region.loc[DATA_region['Year'] > 1979, 'vul_func11yr'] = vul_func11yr
        DATA_region.loc[DATA_region['Year'] > 1979, 'vul_funcSSA5'] = ssa_5

        tot_obs_dam, tot_mod_dam, tot_pred_dam = total_damages(DATA_region[DATA_region['Year'] > 1979])

        annual_mean = DATA_region.loc[DATA_region['Year'] > 1979,
                                              'natcat_flood_damages_2005_CPI'].mean()

        annual_std = DATA_region.loc[DATA_region['Year'] > 1979,
                                             'natcat_flood_damages_2005_CPI'].std()

        vul_mean = DATA_region.loc[DATA_region['Year'] > 1979, 'ratios'].mean()

        p_corr_pred = pears_corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                                     'natcat_flood_damages_2005_CPI'],
                                     DATA_region.loc[DATA_region['Year'] > 1979,
                                                     'Impact_Pred'],
                                     use_log=False)

        p_corr_mod = pears_corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                                    'natcat_flood_damages_2005_CPI'],
                                    DATA_region.loc[DATA_region['Year'] > 1979,
                                                    'Impact_2y_Flopros'],
                                    use_log=False)

        p_corr_clim = pears_corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                     'natcat_flood_damages_2005_CPI'],
                                     DATA_region.loc[DATA_region['Year'] > 1979,
                                     'ImpFix_2y_Flopros'],
                                     use_log=False)

        corr_norm_pred, corr_norm_mod = normalised_corr(DATA_region[DATA_region['Year'] > 1979],
                                                        tot_mod_dam,
                                                        tot_pred_dam)

        fit_info = arange_fit_info(test_region,
                                   p_corr_mod, p_corr_pred, p_corr_clim,
                                   tot_obs_dam, tot_mod_dam, tot_pred_dam,
                                   annual_mean, annual_std, vul_mean, nan_ev)

        region_data = region_data.append(DATA_region, ignore_index=True)
        fit_data = fit_data.append(fit_info, ignore_index=True)

    return region_data, fit_data


DATA = pd.read_csv('/home/insauer/projects/NC_Submission/Data/postprocessing/ModelMedianRegions.csv')

region_names = {'NAM': 'North America',
                'LAM': 'Central America',
                'EUR': 'Western Europe',
                'NAFARA': 'North Africa + Middle East',
                'SSAF': 'SSA + Southern Africa',
                'CAS': 'Central Asia + Eastern Europe',
                'SWEA': 'Southern Asia + South-East Asia',
                'CHN': 'Eastern Asia',
                'AUS': 'Oceania',
                'GLB': 'Global'}

test_regions = list(region_names)


full_ts = add_global(DATA)

region_data_ts, fit_data = vul_fit(full_ts)


region_data_ts.to_csv('/home/insauer/projects/NC_Submission/Data/postprocessing/VulnerabilityAdjustmentTimeSeriesRegions.csv', index=False)

fit_data.to_csv('/home/insauer/projects/NC_Submission/Data/postprocessing/VulnerabilityAdjustmentMetaDataRegions.csv', index=False)

